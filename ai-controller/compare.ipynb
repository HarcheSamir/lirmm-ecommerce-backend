{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf469fc4",
   "metadata": {},
   "source": [
    "### Load the Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#third experiment\n",
    "# --- Configuration ---\n",
    "STATS_DIR = \"prometheus-stats\"\n",
    "\n",
    "def load_and_process_data(filepath, experiment_name, value_upper_bound=10000):\n",
    "    \"\"\"\n",
    "    Loads ANY data CSV, cleans it, and isolates the most recent experiment run.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"ERROR: File not found at {filepath}. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['experiment'] = experiment_name\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Clean the data\n",
    "    df.dropna(subset=['value'], inplace=True)\n",
    "    df = df[df['value'] < value_upper_bound]\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Isolate the most recent run by finding large time gaps\n",
    "    df['time_diff'] = df['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "    new_experiment_start_index = df[df['time_diff'] > 600].index\n",
    "    if not new_experiment_start_index.empty:\n",
    "        last_start_index = new_experiment_start_index[-1]\n",
    "        df = df.loc[last_start_index:]\n",
    "        \n",
    "    return df\n",
    "\n",
    "# --- Load and clean ALL data files for the latest run ---\n",
    "print(\"--- Processing Latency Data ---\")\n",
    "df_latency_ai = load_and_process_data(os.path.join(STATS_DIR, 'ai_run_p99_latency_per_pod_ms.csv'), 'AI-Controlled')\n",
    "df_latency_base = load_and_process_data(os.path.join(STATS_DIR, 'baseline_p99_latency_per_pod_ms.csv'), 'Baseline')\n",
    "\n",
    "print(\"\\n--- Processing RPS Data ---\")\n",
    "# For RPS, the upper bound can be much higher, so we set it higher.\n",
    "df_rps_ai = load_and_process_data(os.path.join(STATS_DIR, 'ai_run_rps_per_pod.csv'), 'AI-Controlled', value_upper_bound=50000)\n",
    "df_rps_base = load_and_process_data(os.path.join(STATS_DIR, 'baseline_rps_per_pod.csv'), 'Baseline', value_upper_bound=50000)\n",
    "\n",
    "print(\"\\nAll dataframes (df_latency_*, df_rps_*) are now loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681ef13",
   "metadata": {},
   "source": [
    "### Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a613da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker # Import the ticker library for formatting\n",
    "\n",
    "# --- Combine the clean latency data ---\n",
    "combined_clean_latency = pd.concat([df_latency_base, df_latency_ai])\n",
    "\n",
    "# --- Aggregate and Normalize Latency Data ---\n",
    "system_latency_avg = combined_clean_latency.groupby(['experiment', pd.Grouper(key='timestamp', freq='15s')])['value'].mean().reset_index()\n",
    "start_times = system_latency_avg.groupby('experiment')['timestamp'].transform('min')\n",
    "system_latency_avg['elapsed_minutes'] = (system_latency_avg['timestamp'] - start_times).dt.total_seconds() / 60\n",
    "\n",
    "# --- Plot the Latency Graph ---\n",
    "# Get the axis object to manipulate it directly\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=system_latency_avg, \n",
    "    x='elapsed_minutes', \n",
    "    y='value', \n",
    "    hue='experiment', \n",
    "    palette=['#ff7f0e', '#1f7b84'], \n",
    "    marker='o', \n",
    "    markersize=5,\n",
    "    linewidth=2.5,\n",
    "    ax=ax  # Tell seaborn to use our axis object\n",
    ")\n",
    "\n",
    "# --- NEW: TAKE MANUAL CONTROL OF THE Y-AXIS ---\n",
    "# 1. Set the scale to logarithmic\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 2. Force the labels to be normal numbers (e.g., \"500\" instead of \"10^2.7\")\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.yaxis.set_minor_formatter(ticker.ScalarFormatter())\n",
    "\n",
    "# 3. Set specific tick marks to guide the eye. You can customize this list.\n",
    "ax.set_yticks([100, 200, 300, 500, 700, 1000, 2000, 3000, 5000])\n",
    "# --- END OF NEW CODE ---\n",
    "\n",
    "\n",
    "# --- Add Titles and Labels ---\n",
    "ax.set_title('Normalized System P99 Latency: Baseline vs. AI-Controlled (Log Scale)', fontsize=20, weight='bold')\n",
    "ax.set_xlabel('Time into Experiment (Minutes)', fontsize=16)\n",
    "ax.set_ylabel('Average P99 Latency (ms) - Log Scale', fontsize=16)\n",
    "ax.legend(title='Experiment Type', fontsize=12)\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_DIR, 'comparison_latency_graph_log_scale_final.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f64df1",
   "metadata": {},
   "source": [
    "###  Compare Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252438ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Summary Statistics from the Cleaned Dataframes ---\n",
    "\n",
    "# Baseline Summary\n",
    "avg_latency_base = df_latency_base['value'].mean()\n",
    "avg_rps_base = df_rps_base['value'].mean()\n",
    "\n",
    "# AI Run Summary\n",
    "avg_latency_ai = df_latency_ai['value'].mean()\n",
    "avg_rps_ai = df_rps_ai['value'].mean()\n",
    "\n",
    "# Calculate Improvements\n",
    "try:\n",
    "    latency_improvement = ((avg_latency_base - avg_latency_ai) / avg_latency_base) * 100\n",
    "except (ZeroDivisionError, ValueError):\n",
    "    latency_improvement = np.nan\n",
    "\n",
    "try:\n",
    "    rps_improvement = ((avg_rps_ai - avg_rps_base) / avg_rps_base) * 100\n",
    "except (ZeroDivisionError, ValueError):\n",
    "    rps_improvement = np.nan\n",
    "\n",
    "# --- Print the Final Report ---\n",
    "print(\"=\"*60)\n",
    "print(\"          PERFORMANCE COMPARISON SUMMARY (Latest Run)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} | {'Baseline':>15} | {'AI-Controlled':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Avg P99 Latency (ms)':<25} | {avg_latency_base:>15.2f} | {avg_latency_ai:>15.2f}\")\n",
    "print(f\"{'Avg Requests Per Second (RPS)':<25} | {avg_rps_base:>15.2f} | {avg_rps_ai:>15.2f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Latency Improvement: {latency_improvement:+.2f}%\")\n",
    "print(f\"RPS Improvement:     {rps_improvement:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
